{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import metrics as smp_metrics\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from shutil import copyfile\n",
    "from copy import deepcopy\n",
    "from PIL import Image, ImageFile\n",
    "from pytorch_lightning import  seed_everything\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from mcode.copy_paste import CopyPaste\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# Setup random seed\n",
    "\n",
    "def set_seed_everything(seed: int):    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    seed_everything(seed)\n",
    "    \n",
    "set_seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/tmp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"True\"\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolypDataset(Dataset):\n",
    "    \"\"\"\n",
    "    dataloader for polyp segmentation tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, image_root, gt_root, trainsize, transform):\n",
    "        self.trainsize = trainsize\n",
    "        self.images = image_root\n",
    "        self.masks = gt_root\n",
    "        self.images = sorted(self.images)\n",
    "        self.masks = sorted(self.masks)\n",
    "        self.filter_files()\n",
    "        self.size = len(self.images)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image = self.rgb_loader(self.images[index])\n",
    "        mask = self.binary_loader(self.masks[index])\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "            mask = mask / 255\n",
    "            \n",
    "        sample = dict(image=image, mask=mask.unsqueeze(0), image_path=self.images[index], mask_path=self.masks[index])\n",
    "        \n",
    "        return sample\n",
    "\n",
    "    def filter_files(self):\n",
    "        assert len(self.images) == len(self.masks)\n",
    "        images = []\n",
    "        masks = []\n",
    "        for img_path, mask_path in zip(self.images, self.masks):\n",
    "            img = Image.open(img_path)\n",
    "            mask = Image.open(mask_path)\n",
    "            if img.size == mask.size:\n",
    "                images.append(img_path)\n",
    "                masks.append(mask_path)\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "    \n",
    "    def rgb_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f).resize((self.trainsize, self.trainsize), Image.Resampling.BILINEAR)\n",
    "            return np.array(img.convert('RGB'))\n",
    "\n",
    "    def binary_loader(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f).resize((self.trainsize, self.trainsize), Image.Resampling.NEAREST)\n",
    "            img = np.array(img.convert('L'))\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter(object):\n",
    "    def __init__(self, num=40):\n",
    "        self.num = num\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        self.losses.append(val)\n",
    "\n",
    "    def show(self):\n",
    "        return torch.mean(torch.stack(self.losses[np.maximum(len(self.losses)-self.num, 0):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsize = 352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/augmentations/transforms.py:1149: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Training labeled with weak augmentation \n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Semi supervised transform for unlabled data with strong augmentation \n",
    "semi_transform = A.Compose(\n",
    "    [\n",
    "    A.Flip(p=0.5),\n",
    "    A.RandomCrop(width=352, height=352),\n",
    "    A.OneOf([A.RandomGamma(), A.RandomBrightness()]),\n",
    "    A.OneOf([A.Blur(), A.GaussianBlur(), A.GlassBlur(), A.MotionBlur(), A.GaussNoise(), A.MedianBlur()]),\n",
    "    A.Cutout(p=0.3, max_h_size=25, max_w_size=25, fill_value=255),\n",
    "    A.ShiftScaleRotate(p=0.3, border_mode=cv2.BORDER_CONSTANT, shift_limit=0.15, scale_limit=0.11),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = PolypDataset(\n",
    "    image_root=glob.glob('/home/nguyen.van.quan/scatsimclr/newdataset/*/image/*'), \n",
    "    gt_root=glob.glob('/home/nguyen.van.quan/scatsimclr/newdataset/*/mask/*'), \n",
    "    trainsize=trainsize, \n",
    "    transform=semi_transform\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), \n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "t_images = glob.glob('/home/nguyen.van.quan/scatsimclr/TestDataset/*/images/*')\n",
    "t_masks = glob.glob('/home/nguyen.van.quan/scatsimclr/TestDataset/*/masks/*')\n",
    "\n",
    "test_dataset = PolypDataset(\n",
    "    image_root=t_images[:100], \n",
    "    gt_root=t_masks[:100], \n",
    "    trainsize=trainsize, \n",
    "    transform=val_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1450\n",
      "Test size: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "n_cpu = os.cpu_count()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(image, mask, original_image=None, original_mask=None):\n",
    "    fontsize = 18\n",
    "    \n",
    "    if original_image is None and original_mask is None:\n",
    "        f, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "        ax[0].imshow(image)\n",
    "        ax[1].imshow(mask)\n",
    "    else:\n",
    "        f, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "\n",
    "        ax[0, 0].imshow(original_image)\n",
    "        ax[0, 0].set_title('Original image', fontsize=fontsize)\n",
    "        \n",
    "        ax[1, 0].imshow(original_mask)\n",
    "        ax[1, 0].set_title('Original mask', fontsize=fontsize)\n",
    "        \n",
    "        ax[0, 1].imshow(image)\n",
    "        ax[0, 1].set_title('Transformed image', fontsize=fontsize)\n",
    "        \n",
    "        ax[1, 1].imshow(mask)\n",
    "        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "CopyPaste requires ['masks', 'paste_image', 'paste_masks']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m unorm(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m mask \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mPolypDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     17\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_loader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks[index])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     image \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m     mask \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/composition.py:205\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/composition.py?line=201'>202</a>\u001b[0m     p\u001b[39m.\u001b[39mpreprocess(data)\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/composition.py?line=203'>204</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(transforms):\n\u001b[0;32m--> <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/composition.py?line=204'>205</a>\u001b[0m     data \u001b[39m=\u001b[39m t(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/composition.py?line=206'>207</a>\u001b[0m     \u001b[39mif\u001b[39;00m check_each_transform:\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/composition.py?line=207'>208</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data_post_transform(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/transforms_interface.py:105\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/transforms_interface.py?line=101'>102</a>\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params()\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/transforms_interface.py?line=103'>104</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets_as_params:\n\u001b[0;32m--> <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/transforms_interface.py?line=104'>105</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(key \u001b[39min\u001b[39;00m kwargs \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets_as_params), \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m requires \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/transforms_interface.py?line=105'>106</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets_as_params\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/transforms_interface.py?line=106'>107</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/transforms_interface.py?line=107'>108</a>\u001b[0m     targets_as_params \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtargets_as_params}\n\u001b[1;32m    <a href='file:///home/nguyen.van.quan/miniconda3/envs/mmseg/lib/python3.9/site-packages/albumentations/core/transforms_interface.py?line=108'>109</a>\u001b[0m     params_dependent_on_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params_dependent_on_targets(targets_as_params)\n",
      "\u001b[0;31mAssertionError\u001b[0m: CopyPaste requires ['masks', 'paste_image', 'paste_masks']"
     ]
    }
   ],
   "source": [
    "sample = train_dataset[2]\n",
    "image = unorm(sample[\"image\"]).permute(1, 2, 0)\n",
    "mask = sample[\"mask\"].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = A.RandomGamma(gamma_limit=(50, 150), always_apply=True, p=0.6)\n",
    "# aug = A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5)\n",
    "augmented = aug(image=image, mask=mask)\n",
    "\n",
    "image_padded = augmented['image']\n",
    "mask_padded = augmented['mask']\n",
    "\n",
    "print(image_padded.shape, mask_padded.shape)\n",
    "\n",
    "visualize(image_padded, mask_padded, original_image=image, original_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, arch='', encoder_name='', in_channels=3, out_classes=1, checkpoint_path='', mm_checkpoint_path='', labeled_dataloader=None, \n",
    "                 unlabeled_dataloader=None, momentum=0.99, use_momentum=False, is_semi=False, is_online=False, teacher=None, use_soft_label=True, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = smp.create_model(\n",
    "            arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
    "        )\n",
    "\n",
    "        # for image segmentation dice loss could be the best first choice\n",
    "        self.loss_fn_1 = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n",
    "        self.loss_fn_2 = active_contour_loss\n",
    "        \n",
    "        # max iou of origin model and momentum model  \n",
    "        self.m_max_iou = 0\n",
    "        self.mm_max_iou = 0\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.mm_checkpoint_path = mm_checkpoint_path\n",
    "        \n",
    "        self.labeled_dataloader = labeled_dataloader\n",
    "        self.unlabeled_dataloader = unlabeled_dataloader\n",
    "        self.momentum = momentum \n",
    "        self.teacher = teacher\n",
    "        self.is_semi = is_semi\n",
    "        self.is_online = is_online\n",
    "        self.use_momentum = use_momentum\n",
    "        self.use_soft_label = use_soft_label\n",
    "        # Beta for unlabeled loss \n",
    "        self.beta = 0.3\n",
    "        \n",
    "        if self.use_momentum:\n",
    "            self.momentum_model = smp.create_model(\n",
    "                arch, encoder_name=encoder_name, in_channels=in_channels, classes=out_classes, **kwargs\n",
    "            )\n",
    "            for param in self.momentum_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            self.momentum_model = None\n",
    "        # Non back-propagate into teacher\n",
    "        if self.is_semi and self.teacher:\n",
    "            for param in self.teacher.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "    def forward(self, image):\n",
    "        return self.model(image)\n",
    "    def mm_forward(self, image):\n",
    "        # normalize image here\n",
    "        mask = self.momentum_model.cuda()(image)\n",
    "        return mask\n",
    "    \n",
    "    def make_pseudo_label(self, image):\n",
    "        logits = self.teacher.cuda()(image)\n",
    "        prob_mask = logits.sigmoid()\n",
    "        # Generate hard label\n",
    "        pred_mask = (prob_mask > 0.5).float()\n",
    "        if self.use_soft_label:\n",
    "            return prob_mask\n",
    "        return pred_mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_momentum_network(self):\n",
    "        \"\"\"Momentum update of the teacher model with student weight\"\"\"\n",
    "        if self.use_momentum and self.momentum_model:\n",
    "            for param_model, param_momentum_model in zip(self.model.parameters(), self.momentum_model.parameters()):\n",
    "                param_momentum_model.data = param_momentum_model.data * self.momentum + param_model.data * (1.0 - self.momentum)\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_teacher_network(self):\n",
    "        \"\"\"Momentum update of the teacher model with student weight\"\"\"\n",
    "        if self.is_semi and self.is_online:\n",
    "            for param_student, param_teacher in zip(self.model.parameters(), self.teacher.parameters()):\n",
    "                param_teacher.data = param_teacher.data * self.momentum + param_student.data * (1.0 - self.momentum)\n",
    "                    \n",
    "    def compute_loss(self, image, mask):\n",
    "        assert image.ndim == 4\n",
    "        assert mask.ndim == 4\n",
    "        \n",
    "        h, w = image.shape[2:]\n",
    "        assert h % 32 == 0 and w % 32 == 0\n",
    "        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n",
    "        assert mask.max() <= 1.0 and mask.min() >= 0\n",
    "        \n",
    "        logits_mask = self.forward(image)\n",
    "        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n",
    "        loss_1 = self.loss_fn_1(logits_mask, mask)\n",
    "        loss_2 = self.loss_fn_2(logits_mask, mask)\n",
    "        \n",
    "        loss = 0.5*loss_1 + 0.5*loss_2\n",
    "        \n",
    "        y_hat_mask = logits_mask.sigmoid().data\n",
    "        y_hat_mask = (y_hat_mask - y_hat_mask.min()) / (y_hat_mask.max() - y_hat_mask.min() + 1e-8)\n",
    "        pred_mask = y_hat_mask.round()\n",
    "        \n",
    "        \n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n",
    "        \n",
    "        result = {\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,\n",
    "            \"loss\": loss\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        # If not have unlabeled data, the batch is only have labeled data\n",
    "        batch_labeled = batch\n",
    "        \n",
    "        if stage == 'train':\n",
    "            batch_labeled = batch['labeled']\n",
    "        \n",
    "        # Batch of labeled\n",
    "        l_image = batch_labeled[\"image\"]\n",
    "        l_mask = batch_labeled[\"mask\"]\n",
    "        \n",
    "        l_result = self.compute_loss(l_image, l_mask)\n",
    "        l_loss = l_result['loss']\n",
    "        # Predefine for unlabeled loss \n",
    "        u_result = l_result\n",
    "        u_result['loss'] = 0\n",
    "        u_loss = u_result['loss']\n",
    "        \n",
    "        # Batch of unlabeled \n",
    "        if self.is_semi and stage == 'train':\n",
    "            batch_unlabeled = batch['unlabeled']\n",
    "            u_image = batch_unlabeled[\"image\"]\n",
    "            # Compute mask from unlabled image with teacher model in online learning\n",
    "            u_mask = self.make_pseudo_label(batch_unlabeled[\"image\"])\n",
    "            u_result = self.compute_loss(u_image, u_mask)\n",
    "            u_loss = u_result['loss']\n",
    "        \n",
    "        # Compute total loss \n",
    "        loss = l_loss + self.beta * u_loss\n",
    "\n",
    "        \n",
    "        # Append loss to result\n",
    "        result = l_result\n",
    "        result['loss'] = loss\n",
    "        \n",
    "        if self.use_momentum:\n",
    "            logits_mm_mask = self.mm_forward(l_image)\n",
    "            \n",
    "            prob_mm_mask = logits_mm_mask.sigmoid().data\n",
    "            prob_mm_mask = (prob_mm_mask - prob_mm_mask.min()) / (prob_mm_mask.max() - prob_mm_mask.min() + 1e-8)\n",
    "            \n",
    "            pred_mm_mask = prob_mm_mask.round()\n",
    "            mm_tp, mm_fp, mm_fn, mm_tn = smp.metrics.get_stats(pred_mm_mask.long(), l_mask.long(), mode=\"binary\")\n",
    "\n",
    "            result['mm_tp'] = mm_tp\n",
    "            result['mm_fp'] = mm_fp\n",
    "            result['mm_fn'] = mm_fn\n",
    "            result['mm_tn'] = mm_tn\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):        \n",
    "        # aggregate step metics\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])    \n",
    "        \n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "        \n",
    "        if self.use_momentum:\n",
    "            # aggregate step metics\n",
    "            tp = torch.cat([x[\"mm_tp\"] for x in outputs])\n",
    "            fp = torch.cat([x[\"mm_fp\"] for x in outputs])\n",
    "            fn = torch.cat([x[\"mm_fn\"] for x in outputs])\n",
    "            tn = torch.cat([x[\"mm_tn\"] for x in outputs])    \n",
    "\n",
    "            mm_per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "            mm_dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "            \n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({'train_dice': dataset_iou})\n",
    "        \n",
    "        if stage == 'valid':\n",
    "            # Save best checkpoint\n",
    "            # if per_image_iou > self.m_max_iou:\n",
    "            print('\\nSave origin model with checkpoint loss = {}'.format(per_image_iou))\n",
    "            self.m_max_iou = per_image_iou\n",
    "            \n",
    "            if self.use_momentum:\n",
    "                if mm_per_image_iou > self.mm_max_iou:\n",
    "                    print('\\nSave momentum model with checkpoint loss = {}'.format(mm_per_image_iou))\n",
    "                    torch.save(self.momentum_model, self.mm_checkpoint_path)\n",
    "                    self.mm_max_iou = mm_per_image_iou\n",
    "                \n",
    "        if self.use_momentum:\n",
    "\n",
    "            metrics = {\n",
    "                f\"{stage}_per_image_iou\": per_image_iou,\n",
    "                f\"{stage}_mm_per_image_iou\": mm_per_image_iou,\n",
    "            }\n",
    "        else:\n",
    "            metrics = {\n",
    "                f\"{stage}_per_image_iou\": per_image_iou,\n",
    "            }\n",
    "        torch.save(self.model, self.checkpoint_path)\n",
    "        \n",
    "        self.log_dict(metrics, prog_bar=True)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        if self.is_semi:\n",
    "            loaders = {\"labeled\": self.labeled_dataloader, \"unlabeled\": self.unlabeled_dataloader}\n",
    "        else:\n",
    "            loaders = {\"labeled\": self.labeled_dataloader}\n",
    "\n",
    "        return loaders\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")            \n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"valid\")\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"valid\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"test\")  \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.shared_epoch_end(outputs, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "    \n",
    "    def on_train_batch_end(self, *args, **kwargs):\n",
    "        \"\"\" Check if we should save a checkpoint after every train batch \"\"\"\n",
    "        global_step = self.global_step\n",
    "        \n",
    "        if global_step % 100 == 0 and self.is_online and self.is_semi:\n",
    "            # Update teacher network \n",
    "            self._update_teacher_network()\n",
    "        \n",
    "        if global_step % 25 == 0 and self.use_momentum:\n",
    "            self._update_momentum_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testdataset(dataset):\n",
    "    return PolypDataset(\n",
    "        image_root=glob.glob('{}/{}/images/*'.format('TestDataset', dataset)), \n",
    "        gt_root=glob.glob('{}/{}/masks/*'.format('TestDataset', dataset)), \n",
    "        trainsize=trainsize, \n",
    "        transform=val_transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.metric import get_scores\n",
    "from tabulate import tabulate\n",
    "def full_val(model, device = 'cuda:1'):\n",
    "    use_wandb = False\n",
    "    print(\"#\" * 20)\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_names = os.listdir('TestDataset/')\n",
    "    table = []\n",
    "    headers = ['Dataset', 'IoU', 'Dice']\n",
    "    ious, dices = AvgMeter(), AvgMeter()\n",
    "\n",
    "    for dataset_name in dataset_names:\n",
    "        tmp_dataset = get_testdataset(dataset_name)\n",
    "        test_loader = DataLoader(tmp_dataset, batch_size=1, shuffle=False, num_workers=n_cpu)   \n",
    "\n",
    "        # print('Dataset_name:', dataset_name)\n",
    "        gts = []\n",
    "        prs = []\n",
    "        for i, pack in enumerate(test_loader, start=1):\n",
    "            image, gt = pack[\"image\"], pack[\"mask\"]\n",
    "            gt = gt[0][0]\n",
    "            gt = np.asarray(gt, np.float32)\n",
    "            image = image.to(device)\n",
    "\n",
    "            res = model(image)[0]\n",
    "            # res = F.interpolate(res, size=gt.shape, mode='bilinear', align_corners=False)\n",
    "            res = res.sigmoid().data.cpu().numpy().squeeze()\n",
    "            res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "            pr = res.round()\n",
    "            gts.append(gt)\n",
    "            prs.append(pr)\n",
    "        mean_iou, mean_dice, _, _ = get_scores(gts, prs)\n",
    "        ious.update(mean_iou)\n",
    "        dices.update(mean_dice)\n",
    "        if use_wandb:\n",
    "            wandb.log({f'{dataset_name}_dice': mean_dice})\n",
    "            wandb.log({f'{dataset_name}_iou': mean_iou})\n",
    "        table.append([dataset_name, mean_iou, mean_dice])\n",
    "    table.append(['Total', ious.avg, dices.avg])\n",
    "\n",
    "    print(tabulate(table, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "    return ious.avg, dices.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when retrain new teacher \n",
    "model = PetModel(\"FPN\", \"densenet169\", in_channels=3, out_classes=1, momentum=0.95,\n",
    "                 labeled_dataloader=train_dataloader, unlabeled_dataloader=None, \n",
    "                 checkpoint_path='runs/checkpoints/fpn_densenet169_full.pth')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\", devices=[0],\n",
    "    max_epochs=100\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model, \n",
    "    val_dataloaders=None\n",
    ") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PetModel(\"FPN\", \"densenet169\", in_channels=3, out_classes=1, checkpoint_path='')\n",
    "model.model = torch.load('runs/checkpoints/fpn_densenet169_full.pth', map_location='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_val(model.model.to('cuda:0'), device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from src.data.augmentor import augmentations\n",
    "\n",
    "image = Image.open('TrainDataset/0/image/415.png').convert('L')\n",
    "img_aug = augmentations(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.polyp_dataset import ActiveDataset\n",
    "from src.data.reconstruct_dataset import SemanticGenesis_Dataset\n",
    "import glob\n",
    "from src.data.augmentor import augmentations\n",
    "train_dataset = ActiveDataset(\n",
    "    image_paths=glob.glob('TrainDataset/*/image/*'), \n",
    "    gt_paths=glob.glob('TrainDataset/*/mask/*'), \n",
    "    trainsize=256, \n",
    "    transform=augmentations\n",
    ")\n",
    "\n",
    "dataset = SemanticGenesis_Dataset(train_dataset, transform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_ori, y_trans = dataset[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b9b35ab1df9b2480d8b627e7b9cec97046ee77a562097ed1d3b1d1b2f663bd17"
  },
  "kernelspec": {
   "display_name": "Python 3.9.15 ('mmseg': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9b35ab1df9b2480d8b627e7b9cec97046ee77a562097ed1d3b1d1b2f663bd17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
